# Towards steering multi-AI interactions with SAEs

This repository contains the code and report for a mechanistic interpretability project involving the use of sparse autoencoders (SAEs) to steer the behavior of LLMs.

**Abstract

There is a famous saying in statistical physics: "More is different." This means that systems of interacting particles often exhibit emergent behaviors that are nearly impossible to predict by studying a single particle. Perhaps a similar rule applies to systems of interacting AI agents. This project works towards the goal of steering the behavior of systems of interacting AI agents using sparse autoencoders (SAEs). The future will likely involve a multitude of AI agents interacting across the internet and economy, so steering these systems' behaviors will be crucial for safety.


* [Link to report (Google Doc)](https://docs.google.com/document/d/1De_swqc0-bT7kcyh1Ok275l3-iD-JY3KYQXpL9p8xgI/edit?usp=sharing)

* Code is contained in steering-AI-conversations-with-SAEs.ipynb. A Google Colab notebook with the code is available [here](https://colab.research.google.com/drive/1A9L63OqIVOriMd2NMAr_RQ0sZtdMpiMU?usp=sharing).